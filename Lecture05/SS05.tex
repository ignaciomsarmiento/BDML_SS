\documentclass[
  shownotes,
  xcolor={svgnames},
  hyperref={colorlinks,citecolor=DarkBlue,linkcolor=andesred,urlcolor=DarkBlue}
  , aspectratio=169]{beamer}
\usepackage{animate}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{mathpazo}
%\usepackage{xcolor}
\usepackage{multimedia}
\usepackage{fancybox}
\usepackage[para]{threeparttable}
\usepackage{multirow}
\setcounter{MaxMatrixCols}{30}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage[compatibility=false,font=small]{caption}
\usepackage{booktabs}
\usepackage{ragged2e}
\usepackage{chronosys}
\usepackage{appendixnumberbeamer}
\usepackage{animate}
\setbeamertemplate{caption}[numbered]
\usepackage{color}
%\usepackage{times}
\usepackage{tikz}
\usepackage{comment} %to comment
%% BibTeX settings
\usepackage{natbib}
\bibliographystyle{apalike}
\bibpunct{(}{)}{,}{a}{,}{,}
\setbeamertemplate{bibliography item}{[\theenumiv]}

% Defines columns for bespoke tables
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


\usepackage{xfrac}


\usepackage{multicol}
\setlength{\columnsep}{0.5cm}

% Theme and colors
\usetheme{Boadilla}

% I define a custom pallete
\definecolor{andesred}{HTML}{1B175E}
\definecolor{andesyellow}{HTML}{ffff00}

% Other options
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\usefonttheme{serif}
\setbeamertemplate{itemize items}[default]
\setbeamertemplate{enumerate items}[square]
\setbeamertemplate{section in toc}[circle]


\definecolor{mybackground}{HTML}{1B175E}
\definecolor{myforeground}{HTML}{0000A0}

\setbeamercolor{normal text}{fg=black,bg=white}
\setbeamercolor{alerted text}{fg=andesred}
\setbeamercolor{example text}{fg=black}

\setbeamercolor{background canvas}{fg=myforeground, bg=white}
\setbeamercolor{background}{fg=myforeground, bg=mybackground}
\setbeamercolor{palette tertiary}{fg=myforeground,bg=mybackground}

\setbeamercolor{palette primary}{fg=black, bg=white}
\setbeamercolor{palette secondary}{fg=black, bg=white!10!andesyellow}
\setbeamercolor{palette tertiary}{fg=black, bg=white}


\setbeamercolor{frametitle}{fg=black}
\setbeamercolor{title}{fg=black}
\setbeamercolor{block title}{fg=andesred}
\setbeamercolor{itemize item}{fg=andesred}
\setbeamercolor{itemize subitem}{fg=andesred}
\setbeamercolor{itemize subsubitem}{fg=andesred}
\setbeamercolor{enumerate item}{fg=andesred}
\setbeamercolor{item projected}{bg=gray!30!white,fg=andesred}
\setbeamercolor{enumerate subitem}{fg=andesred}
\setbeamercolor{section number projected}{bg=gray!30!white,fg=andesred}
\setbeamercolor{section in toc}{fg=andesred}
\setbeamercolor{caption name}{fg=andesred}
\setbeamercolor{button}{bg=gray!30!white,fg=andesred}
\setbeamercolor{title in head/foot}{fg=andesred}



\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter

\makeatother






%%%%%%%%%%%%%%% BEGINS DOCUMENT %%%%%%%%%%%%%%%%%%

\AtBeginSection[]
{
    \begin{frame}
        \frametitle{Agenda}
        \tableofcontents[currentsection]
    \end{frame}
}



\AtBeginSubsection[]
{
    \begin{frame}
        \frametitle{Agenda}
        \tableofcontents[currentsubsection]
    \end{frame}
}

\begin{document}

\title{Selección de Modelos y Regularización}
\subtitle{Big Data y Machine Learning para Economía Aplicada}
\date{}

\author[Sarmiento-Barbieri]{Ignacio Sarmiento-Barbieri}
\institute[Uniandes]{Universidad de los Andes}



%----------------------------------------------------------------------%
\begin{frame}[noframenumbering]
\maketitle
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}
\frametitle{Agenda}

\tableofcontents


\end{frame}
%----------------------------------------------------------------------%
\section{Recap: Predicción y Overfit} 
%----------------------------------------------------------------------%


%----------------------------------------------------------------------%
\begin{frame}[fragile, noframenumbering]
\frametitle{Overfit y Predicción fuera de Muestra}


        \begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.4]{figures/fig_all_sample_polys.pdf}
 \end{figure}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}
\frametitle{Overfit y Predicción fuera de Muestra}



\begin{figure}[H] \centering
            \captionsetup{justification=centering}  
            \includegraphics[scale=.6]{figures/train_test_error.png}
    \end{figure}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}
\frametitle{Overfit y Predicción fuera de Muestra}


\begin{itemize}
  \item ML nos interesa la predicción fuera de muestra
  \pause
  \medskip
  \item Overfit: modelos complejos predicen muy bien dentro de muestra, pero tienden a hacer un mal trabajo fuera de muestra 
  \medskip
  \item Hay que elegir el modelo que ``mejor'' prediga fuera de muestra (out-of-sample)
  \medskip

    \begin{itemize}
    \medskip
    \item Métodos de Remuestreo
    \medskip
      \begin{itemize}

        \item Enfoque del conjunto de validación
        \medskip
        \item LOOCV
        \medskip
        \item Validación cruzada en K-partes (5 o 10)
      \end{itemize}
    \end{itemize}

\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{}
\begin{figure}[H] \centering
  \centering
  \includegraphics[scale=0.35]{figures/baticomputer_meme.jpg}
  \\
  \tiny photo from \url{https://www.dailydot.com/parsec/batman-1966-labels-tumblr-twitter-vine/}
\end{figure}

 \end{frame}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\section{Regularización}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\subsection{Recap: OLS Mechanics}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{OLS 1 Dimension}
\footnotesize
\begin{align}
 min_{\beta} E(\beta)=\sum_{i=1}^n (y_i-x_i \beta)^2 
\end{align}
   \begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.4]{figures/lasso0.pdf}
 \end{figure}

Solution

\begin{align}
\frac{\partial E(\beta)}{\partial \beta}= \sum_{i=1}^n 2(y_i-x_i \beta)(-x_i )
\end{align}

\href{https://cede.uniandes.edu.co/OLS/}{App}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{OLS 1 Dimension}
\begin{align}
\frac{\partial E(\beta)}{\partial \beta}= \sum_{i=1}^n 2(y_i-x_i \beta)(-x_i )
\end{align}

\begin{align}
\hat{\beta} = \frac{\sum y_ix_i}{\sum x^2_i}
\end{align}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{OLS 2 Dimensiones}
\footnotesize
\begin{align}
min_{\beta} E(\beta) = \sum_{i=1}^n (y_i - x_{i1}\beta_1-x_{i2}\beta_2)^2 
\end{align}
\begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.4]{figures/ols1}
 
\tiny
Fuente: \url{https://allmodelsarewrong.github.io}
\end{figure}

\href{https://cede.uniandes.edu.co/OLS/}{App}
\end{frame}
%----------------------------------------------------------------------%
\subsection{Ridge}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge}

\begin{itemize}
\item Para un $\lambda \geq 0$ dado, consideremos ahora el siguiente problema de optimización


\begin{align}
min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta_0 - x_{i1}\beta_1 - \dots - x_{ip}\beta_p)^2 + \lambda \sum_{j=1}^p (\beta_j)^2
\end{align}



\end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge: Intuición en 1 Dimension }

\begin{itemize}
\item 1 predictor estandarizado
\medskip
\item El problema entonces es 
\begin{align}
min_{\beta} E(\beta) = \sum_{i=1}^n (y_i- x_{i}\beta )^2 + \lambda \beta^2
\end{align}

\item La solución?

\vspace{2in}
\href{https://cede.uniandes.edu.co/OLS/}{App}
\end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge: Intuición en 2 Dimensiones }

\begin{itemize}
\item Al problema en 2 dimensiones podemos escribirlo como

  \begin{align}
  min_{\beta} E(\beta) = \sum_{i=1}^n (y_i  - x_{i1}\beta_1  - x_{i2}\beta_2 + \lambda  \left(\beta_1^2 + \beta_2^2 \right)
  \end{align}
\medskip 
\item podemos escribirlo como un problema de optimización restringido
  \begin{align}
     min_{\beta} E(\beta) &= \sum_{i=1}^n (y_i - x_{i1}\beta_1 - x_{i1}\beta_2)^2  \\ \nonumber
     & \text{sujeto a}   \\
     & \left( (\beta_1)^2 + (\beta_2)^2 \right) \leq c \nonumber
  \end{align}

\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge: Intuición en 2 Dimensiones }

\begin{align}
     min_{\beta} E(\beta) &= \sum_{i=1}^n (y_i - x_{i1}\beta_1 - x_{i1}\beta_2)^2  \text{ s.a }   \left( (\beta_1)^2 + (\beta_2)^2 \right) \leq c 
  \end{align}

\begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.3]{figures/ridge1}
 
\tiny
Fuente: \url{https://allmodelsarewrong.github.io}
\end{figure}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge: Intuición en 2 Dimensiones }

\begin{align}
     min_{\beta} E(\beta) &= \sum_{i=1}^n (y_i - x_{i1}\beta_1 - x_{i1}\beta_2)^2  \text{ s.a }   \left( (\beta_1)^2 + (\beta_2)^2 \right) \leq c 
  \end{align}

\begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.3]{figures/ridge2}
 
\tiny
Fuente: \url{https://allmodelsarewrong.github.io}
\end{figure}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge: Intuición en 2 Dimensiones }

\begin{align}
     min_{\beta} E(\beta) &= \sum_{i=1}^n (y_i - x_{i1}\beta_1 - x_{i1}\beta_2)^2  \text{ s.a }   \left( (\beta_1)^2 + (\beta_2)^2 \right) \leq c 
  \end{align}

\begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.3]{figures/ridge3}
 
\tiny
Fuente: \url{https://allmodelsarewrong.github.io}
\end{figure}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge: Intuición en 2 Dimensiones }

\begin{align}
     min_{\beta} E(\beta) &= \sum_{i=1}^n (y_i - x_{i1}\beta_1 - x_{i1}\beta_2)^2  \text{ s.a }   \left( (\beta_1)^2 + (\beta_2)^2 \right) \leq c 
  \end{align}

\begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.3]{figures/ridge4}
 
\tiny
Fuente: \url{https://allmodelsarewrong.github.io}
\end{figure}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Formalmente}



\begin{itemize}
  \item Las $Xs$ están centradas, media cero.
  \medskip
  \item OLS 
  \begin{itemize}
    \item Regresión Simple $y= \beta x + u$ 
        \begin{equation*}
          \hat{\beta}=\frac{\sum x_iy_i }{\sum x_i^2 }
        \end{equation*}
        \medskip
    \item Regresión múltiple y álgebra matricial
    \begin{equation*}
    \hat{\beta}_{OLS}  = (X'X)^{-1} X' y
    \end{equation*}
    
  \end{itemize}
  \item Ridge 
    \begin{equation*}
      \hat{\beta}_{ridge} = (X'X+\lambda I_p)^{-1} X' y
    \end{equation*}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Formalmente}
 \begin{equation*}
      \hat{\beta}_{ridge} = (X'X+\lambda I_p)^{-1} X' y
    \end{equation*}

\begin{itemize}
  \item Ridge es sesgado $E(\hat{\beta}_{ridge}) \neq \beta$
  \medskip
  \item Pero la varianza es menor que la de OLS
  \medskip
  \item Para ciertos valores del parámetro $\lambda$ $MSE_{OLS}>MSE_{ridge}$
\end{itemize}



\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Consequencias: Ridge no es invariante a las escala}
\begin{itemize}
  \item La escala de las variables importa en Ridge, mientras que en OLS no.
  \medskip
  \item Supongamos $z=c*x$
  \item Vamos a mostrar que $\hat{y}^z_{i}=\hat{y}^x_{i}$
  \item Partamos de hacer la regression
  \begin{align}
  y_{i}=\beta^z_{0}+\beta^z_{1}z_i+u
  \end{align}
  \begin{align}
  \hat{\beta}^z_{1} = \frac{\sum(z_{i}-\bar{z})(y_{i}-\bar{y})}{\sum(z_{i}-\bar{z})^{2}}
  \end{align}
  \item Pero $z=c*x$
  \begin{align}
  \hat{\beta}^z_{1} &= \frac{\sum(cx_{i}-c\bar{x})(y_{i}-\bar{y})}{\sum(cx_{i}-\bar{cx})^{2}} \\
                    &= \frac{1}{c}\frac{\sum(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum(x_{i}-\bar{x})^{2}} \\
                    &= \frac{1}{c}\hat{\beta}^x_1
  \end{align}

\end{itemize}
  \end{frame}

%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Consequencias: Ridge no es invariante a las escala}
\begin{itemize}
  \item Entonces
  \begin{align}
  \hat{\beta}^z_{1}z_i &=\hat{\beta}^z_{1}cx_i \\
  &=\frac{1}{c}\hat{\beta}^x_1cx_i \\
  &=\hat{\beta}^x_1 x_i \\
  \end{align}
  
  
\end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Consequencias: Ridge no es invariante a las escala}
\begin{itemize}
  
  \item En términos generales, si $Z=cX$
  \begin{align*}
    \hat{\beta}^Z_{OLS}  &= (Z'Z)^{-1} Z' y \\
                         &= ((cX)'(cX))^{-1} (cX)' y \\
                         &= \frac{c}{c^2}(X'X)^{-1} X' y \\
                         &= \frac{1}{c}(X'X)^{-1} X' y  \\
                         &= \frac{1}{c} \hat{\beta}^X_{OLS}
  \end{align*} 
\end{itemize}

  \end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Consequencias: Ridge no es invariante a las escala}
\begin{itemize}
  \item Entonces
  \begin{align*}
  \hat{\beta}^Z_{OLS} Z  &=  \frac{1}{c} \hat{\beta}^X_{OLS} cX \\
                         &= \hat{\beta}^X_{OLS} X
  \end{align*} 
\medskip
  \item Con Ridge esto no funciona
    \begin{align*}
  \hat{\beta}^Z_{Ridge} Z &\neq  \hat{\beta}^X_{Ridge} X
  \end{align*} 

\item Es importante estandarizar las variables (la mayoría de los softwares lo hace automáticamente)
\end{itemize}


\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Example: Ridge}
\begin{figure}[H] \centering
  \centering
  \includegraphics[scale=0.35]{figures/baticomputer_meme.jpg}
  \\
  \tiny photo from \url{https://www.dailydot.com/parsec/batman-1966-labels-tumblr-twitter-vine/}
\end{figure}

 \end{frame}


%----------------------------------------------------------------------%
\end{document}
