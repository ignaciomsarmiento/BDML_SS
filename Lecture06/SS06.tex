\documentclass[
  shownotes,
  xcolor={svgnames},
  hyperref={colorlinks,citecolor=DarkBlue,linkcolor=andesred,urlcolor=DarkBlue}
  , aspectratio=169]{beamer}
\usepackage{animate}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{mathpazo}
%\usepackage{xcolor}
\usepackage{multimedia}
\usepackage{fancybox}
\usepackage[para]{threeparttable}
\usepackage{multirow}
\setcounter{MaxMatrixCols}{30}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage[compatibility=false,font=small]{caption}
\usepackage{booktabs}
\usepackage{ragged2e}
\usepackage{chronosys}
\usepackage{appendixnumberbeamer}
\usepackage{animate}
\setbeamertemplate{caption}[numbered]
\usepackage{color}
%\usepackage{times}
\usepackage{tikz}
\usepackage{comment} %to comment
%% BibTeX settings
\usepackage{natbib}
\bibliographystyle{apalike}
\bibpunct{(}{)}{,}{a}{,}{,}
\setbeamertemplate{bibliography item}{[\theenumiv]}

% Defines columns for bespoke tables
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


\usepackage{xfrac}


\usepackage{multicol}
\setlength{\columnsep}{0.5cm}

% Theme and colors
\usetheme{Boadilla}

% I define a custom pallete
\definecolor{andesred}{HTML}{1B175E}
\definecolor{andesyellow}{HTML}{ffff00}

% Other options
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\usefonttheme{serif}
\setbeamertemplate{itemize items}[default]
\setbeamertemplate{enumerate items}[square]
\setbeamertemplate{section in toc}[circle]


\definecolor{mybackground}{HTML}{1B175E}
\definecolor{myforeground}{HTML}{0000A0}

\setbeamercolor{normal text}{fg=black,bg=white}
\setbeamercolor{alerted text}{fg=andesred}
\setbeamercolor{example text}{fg=black}

\setbeamercolor{background canvas}{fg=myforeground, bg=white}
\setbeamercolor{background}{fg=myforeground, bg=mybackground}
\setbeamercolor{palette tertiary}{fg=myforeground,bg=mybackground}

\setbeamercolor{palette primary}{fg=black, bg=white}
\setbeamercolor{palette secondary}{fg=black, bg=white!10!andesyellow}
\setbeamercolor{palette tertiary}{fg=black, bg=white}


\setbeamercolor{frametitle}{fg=black}
\setbeamercolor{title}{fg=black}
\setbeamercolor{block title}{fg=andesred}
\setbeamercolor{itemize item}{fg=andesred}
\setbeamercolor{itemize subitem}{fg=andesred}
\setbeamercolor{itemize subsubitem}{fg=andesred}
\setbeamercolor{enumerate item}{fg=andesred}
\setbeamercolor{item projected}{bg=gray!30!white,fg=andesred}
\setbeamercolor{enumerate subitem}{fg=andesred}
\setbeamercolor{section number projected}{bg=gray!30!white,fg=andesred}
\setbeamercolor{section in toc}{fg=andesred}
\setbeamercolor{caption name}{fg=andesred}
\setbeamercolor{button}{bg=gray!30!white,fg=andesred}
\setbeamercolor{title in head/foot}{fg=andesred}



\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter



\makeatother






%%%%%%%%%%%%%%% BEGINS DOCUMENT %%%%%%%%%%%%%%%%%%

\AtBeginSection[]
{
    \begin{frame}
        \frametitle{Agenda}
        \tableofcontents[currentsection]
    \end{frame}
}



\AtBeginSubsection[]
{
    \begin{frame}
        \frametitle{Agenda}
        \tableofcontents[currentsubsection]
    \end{frame}
}

\begin{document}

\title{Selección de Modelos y Regularización}
\subtitle{Big Data y Machine Learning para Economía Aplicada}
\date{}

\author[Sarmiento-Barbieri]{Ignacio Sarmiento-Barbieri}
\institute[Uniandes]{Universidad de los Andes}



%----------------------------------------------------------------------%
\begin{frame}[noframenumbering]
\maketitle
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}
\frametitle{Agenda}

\tableofcontents


\end{frame}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\section{Ridge}
%----------------------------------------------------------------------%
\begin{frame}<1>[label=motivacion]
\frametitle{Regularización: Motivación}

\begin{itemize}
\item Las técnicas econometricas estándar no están optimizadas para la predicción porque se enfocan en la insesgadez.
\medskip
\item OLS por ejemplo es el mejor estimador lineal {\it insesgado}
\medskip
\item OLS minimiza el error {\it ``dentro de muestra''}, eligiendo $\beta$ de forma tal que 


\begin{align}
min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta_0 - x_{i1}\beta_1 - \dots - x_{ip}\beta_p)^2 
\end{align}

\item pero para predicción, no estamos interesados en hacer un buen trabajo dentro de muestra 
\medskip
\item Queremos hacer un buen trabajo, {\bf fuera de muestra}
\end{itemize}
\end{frame}


%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\subsection{Trade-off Sesgo-Varianza}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge}
  \begin{itemize}
    \item Asegurar cero sesgo dentro de muestra crea problemas fuera de muestra: trade-off Sesgo-Varianza
    \medskip
    \item Las técnicas de machine learning fueron desarrolladas para hacer este trade-off de forma empírica.
    \medskip
    \item Vamos a proponer modelos del estilo


\begin{align}
min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta_0 - x_{i1}\beta_1 - \dots - x_{ip}\beta_p)^2 + \lambda \sum_{j=1}^p R(\beta_j)
\end{align}

\item donde $R$ es un regularizador que penaliza funciones que crean varianza
\medskip
\item Explícitamente en la minimización incluimos un termino de sesgo y un termino de varianza.


  \end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge}

\begin{itemize}
\item Para un $\lambda \geq 0$ dado, consideremos ahora el siguiente problema de optimización


\begin{align}
min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta_0 - x_{i1}\beta_1 - \dots - x_{ip}\beta_p)^2 + \lambda \sum_{j=1}^p (\beta_j)^2
\end{align}



\end{itemize}


\end{frame}


%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Formalmente}



\begin{itemize}
  \item Las $Xs$ estan estandarizadas ($x_i$ con media 0 ($\bar{x}=0$) y varianza 1 ($\sum x^2_i=1$))
  \medskip
  \item Regresión: $y= \beta x + u$ 
  \medskip
  \item OLS 
  
        \begin{equation*}
          \hat{\beta}_{ols}=\sum x_iy_i
        \end{equation*}
        \medskip
  
  \item Ridge 
  \begin{equation*}
          \hat{\beta}_{ridge}=\frac{\sum x_iy_i}{(1+\lambda)}=\frac{\hat{\beta}_{ols}}{(1+\lambda)}
        \end{equation*}

\end{itemize}

\href{https://cede.uniandes.edu.co/OLS/}{App}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Formalmente}

\begin{itemize}
    \item En regresión multiple ($X$ es una matriz $n\times k$)
    \medskip
\item Regresión: $y= X \beta   + u$ 
  \medskip
  \item OLS 
  
        \begin{equation*}
          \hat{\beta}_{ols}= (X'X)^{-1}X'y
        \end{equation*}
        \medskip
  
  \item Ridge 
  \begin{equation*}
          \hat{\beta}_{ridge}= (X'X+ \lambda I)^{-1}X'y
        \end{equation*}

\end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge vs OLS}
 
\bigskip
\begin{itemize}
  \item Ridge es sesgado $E(\hat{\beta}_{ridge}) \neq \beta$
  \medskip
  \item Pero la varianza es menor que la de OLS
  \medskip
  \item Para ciertos valores del parámetro $\lambda$ $MSE_{OLS}>MSE_{ridge}$
  \medskip
  \item Mostremos esto para el caso de 1 variable
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge vs OLS}

\begin{itemize}
    \item OLS:
    \begin{itemize}
        \item Sesgo $E(\hat{\beta}_{ols})-\beta=$
        \medskip
        \item Varianza $V(\hat{\beta}_{ols})=$
        \medskip
        \item $MSE(\hat{\beta}_{ols})=$
    \end{itemize}
    \bigskip
\item Ridge:
    \begin{itemize}
        \item Sesgo $E(\hat{\beta}_{ridge})-\beta=$
        \medskip
        \item Varianza $V(\hat{\beta}_{ridge})=$
        \medskip
        \item $MSE(\hat{\beta}_{ridge})=$
    \end{itemize}
\end{itemize}

\end{frame}


%----------------------------------------------------------------------%
\begin{frame}[t]
\frametitle{Ridge vs OLS}
\begin{align}
MSE(\hat{\beta}_{ols})-MSE(\hat{\beta}_{ridge})=
\end{align}


\end{frame}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\subsection{Escala de las variables}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Escala de las variables}

\begin{itemize}
\item La escala de las variables importa en Ridge, mientras que en OLS no.
\medskip
\item Tiene consecuencias 
\begin{itemize}
    \medskip
    \item En  la solución ($\hat{\beta}$)
    \medskip
    \item En la predicción ($\hat{y}$)
\end{itemize}
\end{itemize}

\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Escala de las variables}
\framesubtitle{Ridge no es invariante a las escala}

\begin{itemize}

  \item Supongamos $z=c*x$
  \medskip
  \item Vamos a mostrar que $\hat{y}^z_{i}=\hat{y}^x_{i}$
  \medskip
  \item Partamos del modelo
  \begin{align}
  y_{i}=\beta^z_{0}+\beta^z_{1}z_i+u
  \end{align}
    \begin{align}
    \hat{\beta}^z_{1} = \frac{\sum(z_{i}-\bar{z})(y_{i}-\bar{y})}{\sum(z_{i}-\bar{z})^{2}}
  \end{align}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Escala de las variables}
\framesubtitle{Ridge no es invariante a las escala}

\begin{itemize}
\item Continuando
  \begin{align}
  \hat{\beta}^z_{1} = \frac{\sum(z_{i}-\bar{z})(y_{i}-\bar{y})}{\sum(z_{i}-\bar{z})^{2}}
  \end{align}
  \item Pero $z=c*x$
  \begin{align}
  \hat{\beta}^z_{1} &= \frac{\sum(cx_{i}-c\bar{x})(y_{i}-\bar{y})}{\sum(cx_{i}-\bar{cx})^{2}} \\
                    &= \frac{1}{c}\frac{\sum(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum(x_{i}-\bar{x})^{2}} \\
                    &= \frac{1}{c}\hat{\beta}^x_1
  \end{align}


\item En Ridge?
\end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[t]
\frametitle{Escala de las variables}
\framesubtitle{Ridge no es invariante a las escala}

\begin{itemize}
\item Para un $\lambda \geq 0$ dado,  el problema de optimización


\begin{align}
min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta^z_{0}-\beta^z_{1}z_i)^2 + \lambda (\beta^z_{1})^2
\end{align}
\vspace{3.5cm}
\item Demo: \href{}{baticomputer}, math: Homework
\end{itemize}
\end{frame}


%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Escala de las variables}
\framesubtitle{Ridge no es invariante a las escala}
\begin{itemize}
  \item En la predicción
  

  \begin{align}
  \hat{\beta}^z_{1}z_i &=\hat{\beta}^z_{1}cx_i 
\end{align}
    \begin{align}
  =\frac{1}{c}\hat{\beta}^x_1cx_i 
  \end{align}
\begin{align}
  =\hat{\beta}^x_1 x_i
  \end{align}
  
  
\end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Escala de las variables}
\framesubtitle{Ridge no es invariante a las escala}
\begin{itemize}
  
  \item En términos generales, si $Z=cX$
  \begin{align*}
    \hat{\beta}^Z_{OLS}  &= (Z'Z)^{-1} Z' y \\
                         &= ((cX)'(cX))^{-1} (cX)' y \\
                         &= \frac{c}{c^2}(X'X)^{-1} X' y \\
                         &= \frac{1}{c}(X'X)^{-1} X' y  \\
                         &= \frac{1}{c} \hat{\beta}^X_{OLS}
  \end{align*} 
\end{itemize}

  \end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Escala de las variables}
\framesubtitle{Ridge no es invariante a las escala}
\begin{itemize}
  \item Entonces
  \begin{align*}
  \hat{\beta}^Z_{OLS} Z  &=  \frac{1}{c} \hat{\beta}^X_{OLS} cX \\
                         &= \hat{\beta}^X_{OLS} X
  \end{align*} 
\medskip
  \item Con Ridge esto no funciona
    \begin{align*}
  \hat{\beta}^Z_{Ridge} Z &\neq  \hat{\beta}^X_{Ridge} X
  \end{align*} 

\item Es importante estandarizar las variables (la mayoría de los softwares lo hace automáticamente)
\end{itemize}


\end{frame}


%----------------------------------------------------------------------%
\subsection{More predictors than observations }
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{More predictors than observations ($k>n$)}

\begin{itemize}
  \item What happens when we have more predictors than observations ($k>n$)?
  \bigskip
  \begin{itemize}
   \item OLS fails
   \medskip
   \item Ridge ?
  \end{itemize}

\end{itemize}

\end{frame}


%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{OLS when $k>n$}

\begin{itemize}
  \item Rank? Max number of rows or columns that are linearly independent
  \begin{itemize}
  \item Implies $rank(X_{k\times n}) \leq min(k,n)$
  \end{itemize}
  \bigskip
  \item MCO we need $rank(X_{k\times n})=k \implies k\leq n$
  \bigskip
  \item If $rank(X_{k\times n})=k$ then $rank(X'X)=k$
  \bigskip
  \item If $k>n$, then  $rank(X'X)\leq n < k$ then $(X'X)$ cannot be inverted
  \bigskip
  \item Ridge and Lasso work when $k \geq n$
\end{itemize}

\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge when $k>n$}

\begin{align}
min_{\beta} R(\beta) = \sum_{i=1}^n (y-x\beta)^2 + \lambda  (\beta)^2
\end{align}

\bigskip
\begin{itemize}
  \item Solution $\rightarrow$ data augmentation
  \medskip
  \item Intuition: Ridge ``adds'' $k$ additional points.
  \medskip
  \item Allows us to ``deal'' with $k\geq n$
\end{itemize}
\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[t]
\frametitle{Ridge when $k>n$}

\begin{align}
min_{\beta} R(\beta) = \sum_{i=1}^n (y_i-x_i'\beta)^2 + \lambda  (\beta_s)^2
\end{align}


\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[t]
\frametitle{Ridge when $k>n$}

\begin{align}
min_{\beta} R(\beta) = \sum_{i=1}^n (y_i-\sum^k_{j=1}x_{ij}'\beta_j)^2 + \lambda  (\sum^k_{j=1} \beta_j)^2
\end{align}

\end{frame}

%----------------------------------------------------------------------%
\subsection{Selección de $\lambda$}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Selección de $\lambda$ }


\begin{itemize}
    \item Asegurar cero sesgo dentro de muestra crea problemas fuera de muestra: trade-off Sesgo-Varianza
    \medskip
    \item Ridge hace este trade-off de forma empírica.
    \medskip
    \begin{align}
            min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta_0 - x_{i1}\beta_1 - \dots - x_{ip}\beta_p)^2 + \lambda \sum_{j=1}^p R(\beta_j)
    \end{align}
\item $\lambda$ es el precio al que hacemos este trade off
\medskip
 \item Como elegimos $\lambda$?

 \end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Selección de $\lambda$ }



\begin{itemize}

 \item $\lambda$ es un hiper-parámetro y lo elegimos usando validación cruzada
\medskip
    \begin{itemize}
     \item Partimos la muestra de entrenamiento en K Partes: 

     $MUESTRA= M_{fold\,1} \cup M_{fold\,2} \dots \cup M_{fold\,K}$
     \medskip
     \item Cada conjunto  $M_{fold\,K}$ va a jugar el rol de una muestra de evaluación $M_{eval\,k}$.  
     \medskip
     \item Entonces para cada muestra
             \begin{itemize}
          \item  $M_{train-1}=M_{train} - M_{fold\,1}$
          \medskip
          \item    $\vdots$
          \medskip
          \item   $M_{train-k}=M_{train} - M_{fold\,k}$
         \end{itemize} 
    \end{itemize}  
 \end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Selección de $\lambda$ }

  \begin{itemize}
  \item Luego hacemos el siguiente loop
      \medskip
      \begin{itemize}
          \item Para $i=0,0.001,0.002,\dots,\lambda_{max}$ $\{$ \\
          \medskip
               - Para $k=1,\dots,K$ $\{$ \\
               \medskip
               $\,\,\,\,\,\,$- Ajustar el modelo $m_{i,k}$ con $\lambda_i$ en $M_{train-k}$ \\
                     \medskip
               $\,\,\,\,\,\,$- Calcular y guardar el $MSE(m_{i,k})$ usando $M_{eval-k}$ \\
               \medskip
               $\}$ {\it \# fin para k} \\
               \medskip
               - Calcular y guardar $MSE_i=\frac{1}{K} MSE(m_{i,k})$ \\
               \medskip
          $\}$  {\it \# fin para $\lambda$}
         \medskip
      \end{itemize}
  \medskip
     \item Encontramos el menor $MSE_i$ y usar ese $\lambda_i=\lambda^*$
 
 \end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{}
\begin{figure}[H] \centering
  \centering
  \includegraphics[scale=0.35]{figures/baticomputer_meme.jpg}
  \\
  \tiny photo from \url{https://www.dailydot.com/parsec/batman-1966-labels-tumblr-twitter-vine/}
\end{figure}

 \end{frame}
%----------------------------------------------------------------------%
\section{Lasso}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Lasso}

\begin{itemize}
\item Para un $\lambda \geq 0$ dado, consideremos el siguiente problema de optimización


\begin{align}
min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta_0 - x_{i1}\beta_1 - \dots - x_{ip}\beta_p)^2 + \lambda \sum_{j=1}^p |\beta_j| 
\end{align}

\medskip
\pause
  \begin{itemize}
    \item  ``LASSO's free lunch'': selecciona automáticamente los predictores que van en el modelo ($\beta_j \neq 0$) y los que no   ($\beta_j = 0$)
    \medskip
    \item Por qué? Los coeficientes que no van son soluciones de esquina
    \medskip
    \item  $L(\beta)$ es no differentiable
  \end{itemize}
\end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\section{Recap}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Recap}

\begin{itemize}

    \item El objetivo es predecir bien fuera de muestra, donde nos enfrentamos al trade-off Sesgo-Varianza
    
    \medskip
    \item Propusimos modelos


    \begin{align}
        min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta_0 - x_{i1}\beta_1 - \dots - x_{ip}\beta_p)^2 + \lambda \sum_{j=1}^p R(\beta_j)
    \end{align}
    \begin{itemize}
    \item donde $R$ es un regularizador que penaliza funciones que crean varianza
    \medskip
    \item Explícitamente en la minimización incluimos un termino de sesgo y un termino de varianza.


    \medskip
        \begin{itemize}
            \item Ridge
            \medskip
            \item Lasso
            \medskip
            \item Elastic Net
        \end{itemize}
\end{itemize}

  
    \item Próxima clase: detalles de Lasso y EN
 
\end{itemize}
 

 \end{frame}


%----------------------------------------------------------------------%
\end{document}
